{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMINE = 21\n",
    "SEED = 22\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_as_num(gender):\n",
    "    if gender == \"male\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_group(age): # HIGH NOTE: changing each of the scalars to a vector. This is probably not a good idea\n",
    "    if age < 18:\n",
    "        # 13 - 17\n",
    "        return [1, 0, 0]\n",
    "    elif age < 28:\n",
    "        # 23 - 27\n",
    "        return [0, 1, 0]\n",
    "    elif age < 49:\n",
    "        # 33 - 48\n",
    "        return [0, 0, 1]\n",
    "    else:\n",
    "        return [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kids had a great time at the pool yesterday afternoon splashing around and diving for pennies.  While we were there, a reporter came around from the local paper and was snapping pictures of the kids and taking names.  He took special interest in the fact it was my son's birthday and took some extra pictures of him and his brother.  So, I guess I need to check out this weekend's edition of the paper to see if the kids pictures show up.  The children would get a real kick out of it to see their pictures in the newspaper.    My son also got to take treats to his church group last night and they all sang \"Happy Birthday\" to him - he loved the attention!  The leaders at that group are amazing!  The staff members that are men are wonderful role models for both the children.  Until we meet again......\n",
      "[  255   679  1211     7     1   323     9    14    10 10313   533     4\n",
      "   255    55  1119   604     6    79     4    58   608    18     2   211\n",
      "     2   140     3   434    32    19 15890  3749     6     1   671     3\n",
      "    85    40     1   397   604   262    33     1   605    67    44     5\n",
      "   311  1251    32     6     9     3    85    91   604     7     1  2458\n",
      "    10   983   129    77     3   139  5998     3    58   545   480   102\n",
      "   125     4    36    26  2704   225   533     3    79    27   708     1\n",
      "   860     1  2608    24     8   480    30   653     1  1744  1148     8\n",
      "    30   499    30   741  1638  5324    13   282     1   605   227    23\n",
      "   554   130]\n"
     ]
    }
   ],
   "source": [
    "blog_posts_data_dir = \"data/blogs/json-data/\"\n",
    "train_file_name = \"train.json\"\n",
    "test_file_name = \"test.json\"\n",
    "\n",
    "# Load data\n",
    "with open(blog_posts_data_dir + train_file_name) as r:\n",
    "    training_set = json.load(r)\n",
    "    print(training_set[EXAMINE][\"post\"])\n",
    "\n",
    "median_words_per_sample = np.median([len(instance[\"post\"]) for instance in training_set])\n",
    "\n",
    "# Map each word to a unique int value\n",
    "MAX_WORD_COUNT = 20000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = MAX_WORD_COUNT)\n",
    "posts = [instance[\"post\"] for instance in training_set]\n",
    "tokenizer.fit_on_texts(posts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(posts)\n",
    "median_words_per_tokenized_sample = np.median([len(post) for post in sequences])\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = int(median_words_per_tokenized_sample),\n",
    "                                                     padding = \"post\")\n",
    "for i, instance in enumerate(training_set):\n",
    "    instance[\"post\"] = data[i]\n",
    "    instance[\"gender\"] = get_gender_as_num(instance[\"gender\"])\n",
    "    instance[\"age\"] = get_age_group(int(instance[\"age\"]))\n",
    "print(training_set[EXAMINE][\"post\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = len(training_set)\n",
    "\n",
    "categories_count = len(training_set[0][\"age\"])\n",
    "\n",
    "samples_per_class = {0 : 0, 1 : 0, 2 : 0}\n",
    "for instance in training_set:\n",
    "    for i, a in enumerate(instance[\"age\"]):\n",
    "        if a == 1:\n",
    "            samples_per_class[i] += 1\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 526812\n",
      "Number of Categories: 3\n",
      "Samples per Class: {0: 177940, 1: 250672, 2: 98200}\n",
      "Median Words per Sample: 621.0\n",
      "Median Words per Tokenized Sample: 110.0\n",
      "Samples to Words Per Sample Ratio: 848.328502415459\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\", samples_count)\n",
    "print(\"Number of Categories:\", categories_count)\n",
    "print(\"Samples per Class:\", samples_per_class)\n",
    "print(\"Median Words per Sample:\", median_words_per_sample)\n",
    "print(\"Median Words per Tokenized Sample:\", median_words_per_tokenized_sample)\n",
    "print(\"Samples to Words Per Sample Ratio:\", samples_count / median_words_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(list(length_distribution.keys()))\n",
    "# plt.xlabel(\"Length of a Sample\")\n",
    "# plt.ylabel(\"Number of samples\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "glove_path = \"data/embeddings/glove.6B/\"\n",
    "glove_dict = {}\n",
    "with open(glove_path + \"glove.6B.50d.txt\") as f:\n",
    "    for line in f:\n",
    "        line_values = line.split(\" \")\n",
    "        word = line_values[0]\n",
    "        embedding_coefficients = np.asarray(line_values[1 : ], dtype = \"float32\")\n",
    "        glove_dict[word] = embedding_coefficients\n",
    "\n",
    "glove_weights = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    glove_vector = glove_dict.get(word)\n",
    "    if glove_vector is not None:\n",
    "        glove_weights[i] = glove_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[An Introduction to Different Types of Convolutions](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# Input, Embedding, Conv, Pool, Conv, Pool, Flatten, Dense, Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [glove_weights],\n",
    "                                    input_length = median_words_per_sample, trainable = True))\n",
    "model.add(tf.keras.layers.SeparableConv1D(50, 5, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling1D())\n",
    "model.add(tf.keras.layers.SeparableConv1D(100, 3, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling1D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(24, activation = \"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(3, activation = \"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_train = [instance[\"post\"] for instance in training_set]\n",
    "ages_train = [instance[\"age\"] for instance in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 621, 50)           38312400  \n",
      "_________________________________________________________________\n",
      "separable_conv1d_5 (Separabl (None, 617, 50)           2800      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 308, 50)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_6 (Separabl (None, 306, 100)          5250      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 153, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 15300)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                367224    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 38,687,749\n",
      "Trainable params: 38,687,749\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_3_input to have shape (621,) but got array with shape (110,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-48a5b38cd6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rmsprop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mages_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Machine-Learning-Virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Machine-Learning-Virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Machine-Learning-Virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_3_input to have shape (621,) but got array with shape (110,)"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "model.summary()\n",
    "history = model.fit(np.array(posts_train[:1000]), np.array(ages_train[:1000]), epochs = 10, batch_size = 500, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
